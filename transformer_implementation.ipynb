{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE1jg9kYM38noiRtJACFqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nares10/transformer-implementation/blob/main/transformer_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C4TAipdPxRDr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        \"\"\"\n",
        "        Adds positional information to token embeddings.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create constant 'pe' matrix with values dependent on\n",
        "        # position and dimension\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
        "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [batch_size, seq_len, d_model]\n",
        "        Returns:\n",
        "            Tensor with positional encodings added.\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "1vms2jRUxnWf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "    q, k, v: shape [batch, heads, seq_len, d_k]\n",
        "    mask: mask tensor to prevent attention to certain positions.\n",
        "    \"\"\"\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    attention = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attention, v), attention"
      ],
      "metadata": {
        "id": "cDJm1A_gxyfI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Multi-Head Attention mechanism.\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Define linear layers for queries, keys, and values.\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        # Perform linear projections and split into multiple heads\n",
        "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "        # Compute scaled dot-product attention for each head\n",
        "        x, attn = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # Concatenate heads and put through final linear layer\n",
        "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.out(x)"
      ],
      "metadata": {
        "id": "7FEgeHa8yVzz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Two-layer feed-forward network.\n",
        "        \"\"\"\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "vycNGEboyZk9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Single Encoder Layer composed of multi-head self-attention and feed-forward network.\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention sublayer with residual connection and layer normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        # Feed-forward sublayer with residual connection and layer normalization\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "_4-JBVbmydM4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Single Decoder Layer which includes:\n",
        "          - Self-attention for target sequence.\n",
        "          - Encoder-decoder attention.\n",
        "          - Feed-forward network.\n",
        "        \"\"\"\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        # Self-attention sublayer for target sequence\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
        "        # Encoder-decoder attention sublayer\n",
        "        enc_dec_attn_output = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout2(enc_dec_attn_output))\n",
        "        # Feed-forward sublayer\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm3(x + self.dropout3(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "B8bUDlNYygw7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, dropout=0.1, max_len=5000):\n",
        "        \"\"\"\n",
        "        Transformer model combining encoder and decoder stacks.\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Source tensor [batch_size, src_seq_len]\n",
        "            tgt: Target tensor [batch_size, tgt_seq_len]\n",
        "            src_mask, tgt_mask: Masks to control attention (optional)\n",
        "        Returns:\n",
        "            Output logits of shape [batch_size, tgt_seq_len, tgt_vocab_size]\n",
        "        \"\"\"\n",
        "        # Embedding + positional encoding\n",
        "        src = self.positional_encoding(self.src_embedding(src))\n",
        "        tgt = self.positional_encoding(self.tgt_embedding(tgt))\n",
        "\n",
        "        # Pass through the encoder stack\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, src_mask)\n",
        "        enc_output = src\n",
        "\n",
        "        # Pass through the decoder stack\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Final linear layer for output probabilities\n",
        "        output = self.output_layer(tgt)\n",
        "        return output"
      ],
      "metadata": {
        "id": "RqtHFllwylTj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "src_seq_len = 10\n",
        "tgt_seq_len = 10\n",
        "src_vocab_size = 1000\n",
        "tgt_vocab_size = 1000\n"
      ],
      "metadata": {
        "id": "5yPbB7IRytlj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(src_vocab_size, tgt_vocab_size)"
      ],
      "metadata": {
        "id": "QpGCjruPy2jv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))\n",
        "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))"
      ],
      "metadata": {
        "id": "2TC9CoF9y8D4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(src, tgt)"
      ],
      "metadata": {
        "id": "UegO90CWzPFR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output shape:\", output.shape)  # Expected: [batch_size, tgt_seq_len, tgt_vocab_size]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5UMRybIzP3B",
        "outputId": "91b9c647-287d-4568-ff61-a327af6f11ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 10, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNyVm0LSzU7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}